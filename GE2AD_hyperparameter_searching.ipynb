{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "\n",
    "import pickle\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,recall_score,precision_score,precision_recall_curve,f1_score,auc\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "import scipy.stats as ss\n",
    "random_seed = 3\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN4, self).__init__()\n",
    "        self.dense1 =nn.Sequential(nn.Linear(14040 , int(14040 *1/32)))\n",
    "        self.dense2 =nn.Sequential(nn.Linear(int(14040 *1/32), int(14040 *1/512)))\n",
    "        self.dense4  =nn.Sequential(nn.Linear(int(14040 *1/512),int(14040 *1/8192)))\n",
    "        self.dense6 =nn.Sequential(nn.Linear(int(14040 *1/8192),1),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dense4(X)             \n",
    "        se_predict = self.dense6(X)\n",
    "\n",
    "        return se_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN6, self).__init__()\n",
    "        self.dense1 =nn.Sequential(nn.Linear(14040 , int(14040 *1/16)))\n",
    "        self.dense2 =nn.Sequential(nn.Linear(int(14040 *1/16), int(14040 *1/64)))\n",
    "        self.dense3 =nn.Sequential(nn.Linear(int(14040 *1/64),int(14040 *1/256)))\n",
    "        self.dense4  =nn.Sequential(nn.Linear(int(14040 *1/256),int(14040 *1/1024)))\n",
    "        self.dense5  =nn.Sequential(nn.Linear(int(14040 *1/1024),int(14040 *1/4096)))\n",
    "        self.dense6 =nn.Sequential(nn.Linear(int(14040 *1/4096),1),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dense3(X)\n",
    "        X = self.dense4(X)        \n",
    "        X = self.dense5(X)        \n",
    "        se_predict = self.dense6(X)\n",
    "\n",
    "        return se_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN8, self).__init__()\n",
    "        self.dense0 =nn.Sequential(nn.Linear(14040 , int(14040 *1/2)))\n",
    "        self.dense1 =nn.Sequential(nn.Linear(int(14040 *1/2) , int(14040 *1/8)))\n",
    "        self.dense12 =nn.Sequential(nn.Linear(int(14040 *1/8) , int(14040 *1/32)))\n",
    "        self.dense2 =nn.Sequential(nn.Linear(int(14040 *1/32), int(14040 *1/128)))\n",
    "        self.dense3 =nn.Sequential(nn.Linear(int(14040 *1/128),int(14040 *1/512)))\n",
    "        self.dense4  =nn.Sequential(nn.Linear(int(14040 *1/512),int(14040 *1/2048)))\n",
    "        self.dense5  =nn.Sequential(nn.Linear(int(14040 *1/2048),int(14040 *1/8192)))\n",
    "        self.dense6 =nn.Sequential(nn.Linear(int(14040 *1/8192),1),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense12(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dense3(X)\n",
    "        X = self.dense4(X)        \n",
    "        X = self.dense5(X)        \n",
    "        se_predict = self.dense6(X)\n",
    "\n",
    "        return se_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ANM1 = pd.read_csv('/ANM1_inter_norm.csv',sep=',',index_col=0)\n",
    "ANM2 = pd.read_csv('/ANM2_inter_norm.csv',sep=',',index_col=0)\n",
    "ADNI = pd.read_csv('/ADNI_inter_norm.csv',sep=',',index_col=0)\n",
    "add1 = pd.read_csv('/EW_inter_norm.csv',sep=',',index_col=0)\n",
    "ROS = pd.read_csv('/ROSMAP_inter_norm.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1 = add1.loc[:,add1.loc['DX']!='MCI' ]\n",
    "annot = add1.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_EW = np.ravel(annot, order='F').astype('float32')\n",
    "EW_SNP = add1.loc['APOE4']\n",
    "EW_MMSE= add1.loc['MMSE']\n",
    "EW_value = add1\n",
    "EW_value.drop('APOE4',axis=0,inplace=True)\n",
    "EW_value.drop('DX',axis=0,inplace=True)\n",
    "EW_value.drop('MMSE',axis=0,inplace=True)\n",
    "\n",
    "\n",
    "ADNI = ADNI.loc[:,ADNI.loc['DX']!='MCI' ]\n",
    "ADNI = ADNI.dropna(axis=1)\n",
    "annot = ADNI.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_ADNI = np.ravel(annot, order='F').astype('float32')\n",
    "ADNI_SNP = ADNI.loc['APOE4']\n",
    "ADNI_MMSE= ADNI.loc['MMSE']        \n",
    "ADNI.drop('APOE4',axis=0,inplace=True)\n",
    "ADNI.drop('DX',axis=0,inplace=True)\n",
    "ADNI.drop('MMSE',axis=0,inplace=True)\n",
    "ADNI_value = ADNI\n",
    "\n",
    "ANM1 = ANM1.loc[:,ANM1.loc['DX']!='MCI' ]\n",
    "annot = ANM1.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_ANM1 = np.ravel(annot, order='F').astype('float32')\n",
    "ANM1_SNP = ANM1.loc['APOE4']\n",
    "ANM1_MMSE= ANM1.loc['MMSE']\n",
    "ANM1.drop('APOE4',axis=0,inplace=True)\n",
    "ANM1.drop('MMSE',axis=0,inplace=True)\n",
    "ANM1.drop('DX',axis=0,inplace=True)\n",
    "ANM1_value = ANM1\n",
    "\n",
    "ANM2 = ANM2.loc[:,ANM2.loc['DX']!='MCI' ]\n",
    "annot = ANM2.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_ANM2 = np.ravel(annot, order='F').astype('float32')\n",
    "ANM2.drop('DX',axis=0,inplace=True)\n",
    "ANM2.loc['AGE'] = pd.DataFrame(ANM2.loc['AGE']).fillna(pd.DataFrame(ANM2.loc['AGE']).astype('float').mean(axis=0)).T.loc['AGE']\n",
    "ANM2_SNP = ANM2.loc['APOE4']\n",
    "ANM2_MMSE= ANM2.loc['MMSE']\n",
    "ANM2.drop('APOE4',axis=0,inplace=True)\n",
    "ANM2.drop('MMSE',axis=0,inplace=True)\n",
    "ANM2_value = ANM2\n",
    "\n",
    "ROS = ROS.loc[:,ROS.loc['DX']!='MCI' ]\n",
    "annot = ROS.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "\n",
    "ad_ge_ROS = np.ravel(annot, order='F').astype('float32')\n",
    "ROS_SNP = ROS.loc['APOE4']\n",
    "ROS_MMSE= ROS.loc['MMSE']\n",
    "ROS.drop('APOE4',axis=0,inplace=True)\n",
    "ROS.drop('MMSE',axis=0,inplace=True)\n",
    "ROS.drop('DX',axis=0,inplace=True)\n",
    "ROS_value = ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1.]\n",
      "Epoch [100], AUC: 0.6571\n",
      "Epoch [200], AUC: 0.6800\n",
      "Epoch [300], AUC: 0.6891\n",
      "Epoch [400], AUC: 0.6875\n",
      "Epoch [500], AUC: 0.6844\n",
      "Epoch [600], AUC: 0.6866\n",
      "Epoch [700], AUC: 0.6868\n",
      "Epoch [800], AUC: 0.6856\n",
      "Epoch [900], AUC: 0.6872\n",
      "Epoch [1000], AUC: 0.6889\n",
      "Epoch [1100], AUC: 0.6875\n",
      "Epoch [1200], AUC: 0.6876\n",
      "Epoch [1300], AUC: 0.6866\n",
      "Epoch [1400], AUC: 0.6878\n",
      "Epoch [1500], AUC: 0.6873\n",
      "Epoch [1600], AUC: 0.6879\n",
      "Epoch [1700], AUC: 0.6892\n",
      "Epoch [1800], AUC: 0.6868\n",
      "Epoch [1900], AUC: 0.6868\n",
      "Epoch [2000], AUC: 0.6860\n",
      "Epoch [2100], AUC: 0.6858\n",
      "Epoch [2200], AUC: 0.6859\n",
      "Epoch [2300], AUC: 0.6853\n",
      "Using GE  AUC 0.693\n",
      "1\n",
      "Epoch [100], AUC: 0.6608\n",
      "Epoch [200], AUC: 0.6547\n",
      "Epoch [300], AUC: 0.6809\n",
      "Epoch [400], AUC: 0.6856\n",
      "Epoch [500], AUC: 0.6840\n",
      "Epoch [600], AUC: 0.6894\n",
      "Epoch [700], AUC: 0.6866\n",
      "Epoch [800], AUC: 0.6879\n",
      "Epoch [900], AUC: 0.6905\n",
      "Epoch [1000], AUC: 0.6898\n",
      "Epoch [1100], AUC: 0.6925\n",
      "Epoch [1200], AUC: 0.6910\n",
      "Epoch [1300], AUC: 0.6919\n",
      "Epoch [1400], AUC: 0.6921\n",
      "Epoch [1500], AUC: 0.6911\n",
      "Epoch [1600], AUC: 0.6899\n",
      "Epoch [1700], AUC: 0.6915\n",
      "Epoch [1800], AUC: 0.6903\n",
      "Epoch [1900], AUC: 0.6879\n",
      "Epoch [2000], AUC: 0.6891\n",
      "Epoch [2100], AUC: 0.6878\n",
      "Epoch [2200], AUC: 0.6866\n",
      "Epoch [2300], AUC: 0.6858\n",
      "Epoch [2400], AUC: 0.6870\n",
      "Epoch [2500], AUC: 0.6863\n",
      "Epoch [2600], AUC: 0.6860\n",
      "Epoch [2700], AUC: 0.6850\n",
      "Epoch [2800], AUC: 0.6868\n",
      "Epoch [2900], AUC: 0.6847\n",
      "Epoch [3000], AUC: 0.6840\n",
      "Epoch [3100], AUC: 0.6854\n",
      "Using GE  AUC 0.695\n",
      "2\n",
      "Epoch [100], AUC: 0.6491\n",
      "Epoch [200], AUC: 0.6489\n",
      "Epoch [300], AUC: 0.6714\n",
      "Epoch [400], AUC: 0.6759\n",
      "Epoch [500], AUC: 0.6748\n",
      "Epoch [600], AUC: 0.6773\n",
      "Epoch [700], AUC: 0.6790\n",
      "Epoch [800], AUC: 0.6784\n",
      "Epoch [900], AUC: 0.6804\n",
      "Epoch [1000], AUC: 0.6806\n",
      "Epoch [1100], AUC: 0.6804\n",
      "Epoch [1200], AUC: 0.6810\n",
      "Epoch [1300], AUC: 0.6805\n",
      "Epoch [1400], AUC: 0.6808\n",
      "Epoch [1500], AUC: 0.6830\n",
      "Epoch [1600], AUC: 0.6807\n",
      "Epoch [1700], AUC: 0.6817\n",
      "Epoch [1800], AUC: 0.6792\n",
      "Epoch [1900], AUC: 0.6784\n",
      "Epoch [2000], AUC: 0.6787\n",
      "Epoch [2100], AUC: 0.6780\n",
      "Epoch [2200], AUC: 0.6768\n",
      "Epoch [2300], AUC: 0.6781\n",
      "Epoch [2400], AUC: 0.6787\n",
      "Epoch [2500], AUC: 0.6779\n",
      "Epoch [2600], AUC: 0.6746\n",
      "Epoch [2700], AUC: 0.6769\n",
      "Epoch [2800], AUC: 0.6754\n",
      "Epoch [2900], AUC: 0.6755\n",
      "Epoch [3000], AUC: 0.6746\n",
      "Epoch [3100], AUC: 0.6752\n",
      "Epoch [3200], AUC: 0.6749\n",
      "Using GE  AUC 0.685\n",
      "3\n",
      "Epoch [100], AUC: 0.6304\n",
      "Epoch [200], AUC: 0.6413\n",
      "Epoch [300], AUC: 0.6419\n",
      "Epoch [400], AUC: 0.6419\n",
      "Epoch [500], AUC: 0.6445\n",
      "Epoch [600], AUC: 0.6509\n",
      "Epoch [700], AUC: 0.6591\n",
      "Epoch [800], AUC: 0.6671\n",
      "Epoch [900], AUC: 0.6765\n",
      "Epoch [1000], AUC: 0.6785\n",
      "Epoch [1100], AUC: 0.6782\n",
      "Epoch [1200], AUC: 0.6753\n",
      "Epoch [1300], AUC: 0.6749\n",
      "Epoch [1400], AUC: 0.6783\n",
      "Epoch [1500], AUC: 0.6777\n",
      "Epoch [1600], AUC: 0.6777\n",
      "Epoch [1700], AUC: 0.6775\n",
      "Epoch [1800], AUC: 0.6767\n",
      "Epoch [1900], AUC: 0.6772\n",
      "Epoch [2000], AUC: 0.6778\n",
      "Epoch [2100], AUC: 0.6772\n",
      "Epoch [2200], AUC: 0.6784\n",
      "Epoch [2300], AUC: 0.6772\n",
      "Epoch [2400], AUC: 0.6777\n",
      "Epoch [2500], AUC: 0.6775\n",
      "Epoch [2600], AUC: 0.6774\n",
      "Epoch [2700], AUC: 0.6772\n",
      "Epoch [2800], AUC: 0.6769\n",
      "Epoch [2900], AUC: 0.6762\n",
      "Using GE  AUC 0.680\n",
      "4\n",
      "Epoch [100], AUC: 0.5022\n",
      "Epoch [200], AUC: 0.5364\n",
      "Epoch [300], AUC: 0.5702\n",
      "Epoch [400], AUC: 0.5882\n",
      "Epoch [500], AUC: 0.6025\n",
      "Epoch [600], AUC: 0.6132\n",
      "Epoch [700], AUC: 0.6237\n",
      "Epoch [800], AUC: 0.6316\n",
      "Epoch [900], AUC: 0.6372\n",
      "Epoch [1000], AUC: 0.6420\n",
      "Epoch [1100], AUC: 0.6468\n",
      "Epoch [1200], AUC: 0.6465\n",
      "Epoch [1300], AUC: 0.6455\n",
      "Epoch [1400], AUC: 0.6429\n",
      "Epoch [1500], AUC: 0.6429\n",
      "Epoch [1600], AUC: 0.6482\n",
      "Epoch [1700], AUC: 0.6563\n",
      "Epoch [1800], AUC: 0.6679\n",
      "Epoch [1900], AUC: 0.6789\n",
      "Epoch [2000], AUC: 0.6825\n",
      "Epoch [2100], AUC: 0.6833\n",
      "Epoch [2200], AUC: 0.6777\n",
      "Epoch [2300], AUC: 0.6772\n",
      "Epoch [2400], AUC: 0.6780\n",
      "Epoch [2500], AUC: 0.6782\n",
      "Epoch [2600], AUC: 0.6791\n",
      "Epoch [2700], AUC: 0.6789\n",
      "Epoch [2800], AUC: 0.6793\n",
      "Epoch [2900], AUC: 0.6802\n",
      "Epoch [3000], AUC: 0.6809\n",
      "Epoch [3100], AUC: 0.6806\n",
      "Epoch [3200], AUC: 0.6811\n",
      "Epoch [3300], AUC: 0.6807\n",
      "Epoch [3400], AUC: 0.6817\n",
      "Epoch [3500], AUC: 0.6823\n",
      "Epoch [3600], AUC: 0.6820\n",
      "Epoch [3700], AUC: 0.6823\n",
      "Epoch [3800], AUC: 0.6827\n",
      "Epoch [3900], AUC: 0.6827\n",
      "Epoch [4000], AUC: 0.6827\n",
      "Epoch [4100], AUC: 0.6826\n",
      "Using GE  AUC 0.685\n",
      "5\n",
      "Epoch [100], AUC: 0.5289\n",
      "Epoch [200], AUC: 0.5599\n",
      "Epoch [300], AUC: 0.5821\n",
      "Epoch [400], AUC: 0.5981\n",
      "Epoch [500], AUC: 0.6109\n",
      "Epoch [600], AUC: 0.6189\n",
      "Epoch [700], AUC: 0.6273\n",
      "Epoch [800], AUC: 0.6343\n",
      "Epoch [900], AUC: 0.6379\n",
      "Epoch [1000], AUC: 0.6433\n",
      "Epoch [1100], AUC: 0.6474\n",
      "Epoch [1200], AUC: 0.6478\n",
      "Epoch [1300], AUC: 0.6473\n",
      "Epoch [1400], AUC: 0.6467\n",
      "Epoch [1500], AUC: 0.6475\n",
      "Epoch [1600], AUC: 0.6484\n",
      "Epoch [1700], AUC: 0.6443\n",
      "Epoch [1800], AUC: 0.6457\n",
      "Epoch [1900], AUC: 0.6558\n",
      "Epoch [2000], AUC: 0.6647\n",
      "Epoch [2100], AUC: 0.6699\n",
      "Epoch [2200], AUC: 0.6716\n",
      "Epoch [2300], AUC: 0.6644\n",
      "Epoch [2400], AUC: 0.6598\n",
      "Epoch [2500], AUC: 0.6578\n",
      "Epoch [2600], AUC: 0.6574\n",
      "Epoch [2700], AUC: 0.6578\n",
      "Epoch [2800], AUC: 0.6574\n",
      "Epoch [2900], AUC: 0.6570\n",
      "Epoch [3000], AUC: 0.6565\n",
      "Epoch [3100], AUC: 0.6575\n",
      "Epoch [3200], AUC: 0.6583\n",
      "Epoch [3300], AUC: 0.6574\n",
      "Epoch [3400], AUC: 0.6573\n",
      "Epoch [3500], AUC: 0.6597\n",
      "Epoch [3600], AUC: 0.6583\n",
      "Epoch [3700], AUC: 0.6588\n",
      "Epoch [3800], AUC: 0.6591\n",
      "Epoch [3900], AUC: 0.6590\n",
      "Epoch [4000], AUC: 0.6595\n",
      "Epoch [4100], AUC: 0.6590\n",
      "Using GE  AUC 0.674\n",
      "6\n",
      "Epoch [100], AUC: 0.5737\n",
      "Epoch [200], AUC: 0.3894\n",
      "Epoch [300], AUC: 0.6296\n",
      "Epoch [400], AUC: 0.6328\n",
      "Epoch [500], AUC: 0.6383\n",
      "Epoch [600], AUC: 0.6395\n",
      "Epoch [700], AUC: 0.6394\n",
      "Epoch [800], AUC: 0.6387\n",
      "Epoch [900], AUC: 0.6387\n",
      "Epoch [1000], AUC: 0.6394\n",
      "Epoch [1100], AUC: 0.6401\n",
      "Epoch [1200], AUC: 0.6408\n",
      "Epoch [1300], AUC: 0.6417\n",
      "Epoch [1400], AUC: 0.6416\n",
      "Epoch [1500], AUC: 0.6415\n",
      "Epoch [1600], AUC: 0.6419\n",
      "Epoch [1700], AUC: 0.6425\n",
      "Epoch [1800], AUC: 0.6436\n",
      "Epoch [1900], AUC: 0.6450\n",
      "Epoch [2000], AUC: 0.6452\n",
      "Epoch [2100], AUC: 0.6463\n",
      "Epoch [2200], AUC: 0.6468\n",
      "Epoch [2300], AUC: 0.6476\n",
      "Epoch [2400], AUC: 0.6478\n",
      "Epoch [2500], AUC: 0.6485\n",
      "Epoch [2600], AUC: 0.6504\n",
      "Epoch [2700], AUC: 0.6507\n",
      "Epoch [2800], AUC: 0.6512\n",
      "Epoch [2900], AUC: 0.6526\n",
      "Epoch [3000], AUC: 0.6537\n",
      "Epoch [3100], AUC: 0.6552\n",
      "Epoch [3200], AUC: 0.6561\n",
      "Epoch [3300], AUC: 0.6561\n",
      "Epoch [3400], AUC: 0.6580\n",
      "Epoch [3500], AUC: 0.6600\n",
      "Epoch [3600], AUC: 0.6602\n",
      "Epoch [3700], AUC: 0.6620\n",
      "Epoch [3800], AUC: 0.6633\n",
      "Epoch [3900], AUC: 0.6648\n",
      "Epoch [4000], AUC: 0.6665\n",
      "Epoch [4100], AUC: 0.6671\n",
      "Epoch [4200], AUC: 0.6685\n",
      "Epoch [4300], AUC: 0.6686\n",
      "Epoch [4400], AUC: 0.6692\n",
      "Epoch [4500], AUC: 0.6702\n",
      "Epoch [4600], AUC: 0.6707\n",
      "Epoch [4700], AUC: 0.6710\n",
      "Epoch [4800], AUC: 0.6721\n",
      "Epoch [4900], AUC: 0.6728\n",
      "Epoch [5000], AUC: 0.6734\n",
      "Epoch [5100], AUC: 0.6735\n",
      "Epoch [5200], AUC: 0.6731\n",
      "Epoch [5300], AUC: 0.6742\n",
      "Epoch [5400], AUC: 0.6751\n",
      "Epoch [5500], AUC: 0.6745\n",
      "Epoch [5600], AUC: 0.6736\n",
      "Epoch [5700], AUC: 0.6743\n",
      "Epoch [5800], AUC: 0.6732\n",
      "Epoch [5900], AUC: 0.6728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6000], AUC: 0.6726\n",
      "Epoch [6100], AUC: 0.6723\n",
      "Epoch [6200], AUC: 0.6723\n",
      "Epoch [6300], AUC: 0.6733\n",
      "Epoch [6400], AUC: 0.6728\n",
      "Epoch [6500], AUC: 0.6722\n",
      "Epoch [6600], AUC: 0.6717\n",
      "Epoch [6700], AUC: 0.6713\n",
      "Epoch [6800], AUC: 0.6714\n",
      "Epoch [6900], AUC: 0.6711\n",
      "Epoch [7000], AUC: 0.6719\n",
      "Epoch [7100], AUC: 0.6719\n",
      "Epoch [7200], AUC: 0.6721\n",
      "Epoch [7300], AUC: 0.6719\n",
      "Using GE  AUC 0.675\n",
      "7\n",
      "Epoch [100], AUC: 0.5782\n",
      "Epoch [200], AUC: 0.5583\n",
      "Epoch [300], AUC: 0.5400\n",
      "Epoch [400], AUC: 0.4783\n",
      "Epoch [500], AUC: 0.4926\n",
      "Epoch [600], AUC: 0.5089\n",
      "Epoch [700], AUC: 0.5232\n",
      "Epoch [800], AUC: 0.5343\n",
      "Epoch [900], AUC: 0.5436\n",
      "Epoch [1000], AUC: 0.5532\n",
      "Epoch [1100], AUC: 0.5612\n",
      "Epoch [1200], AUC: 0.5690\n",
      "Epoch [1300], AUC: 0.5763\n",
      "Epoch [1400], AUC: 0.5825\n",
      "Epoch [1500], AUC: 0.5886\n",
      "Epoch [1600], AUC: 0.5932\n",
      "Epoch [1700], AUC: 0.5971\n",
      "Epoch [1800], AUC: 0.6022\n",
      "Epoch [1900], AUC: 0.6068\n",
      "Epoch [2000], AUC: 0.6107\n",
      "Epoch [2100], AUC: 0.6129\n",
      "Epoch [2200], AUC: 0.6159\n",
      "Epoch [2300], AUC: 0.6180\n",
      "Epoch [2400], AUC: 0.6202\n",
      "Epoch [2500], AUC: 0.6227\n",
      "Epoch [2600], AUC: 0.6241\n",
      "Epoch [2700], AUC: 0.6272\n",
      "Epoch [2800], AUC: 0.6274\n",
      "Epoch [2900], AUC: 0.6275\n",
      "Epoch [3000], AUC: 0.6290\n",
      "Epoch [3100], AUC: 0.6307\n",
      "Epoch [3200], AUC: 0.6323\n",
      "Epoch [3300], AUC: 0.6336\n",
      "Epoch [3400], AUC: 0.6348\n",
      "Epoch [3500], AUC: 0.6359\n",
      "Epoch [3600], AUC: 0.6369\n",
      "Epoch [3700], AUC: 0.6375\n",
      "Epoch [3800], AUC: 0.6374\n",
      "Epoch [3900], AUC: 0.6379\n",
      "Epoch [4000], AUC: 0.6384\n",
      "Epoch [4100], AUC: 0.6394\n",
      "Epoch [4200], AUC: 0.6395\n",
      "Epoch [4300], AUC: 0.6399\n",
      "Epoch [4400], AUC: 0.6404\n",
      "Epoch [4500], AUC: 0.6414\n",
      "Epoch [4600], AUC: 0.6413\n",
      "Epoch [4700], AUC: 0.6407\n",
      "Epoch [4800], AUC: 0.6412\n",
      "Epoch [4900], AUC: 0.6416\n",
      "Epoch [5000], AUC: 0.6412\n",
      "Epoch [5100], AUC: 0.6415\n",
      "Epoch [5200], AUC: 0.6413\n",
      "Epoch [5300], AUC: 0.6415\n",
      "Epoch [5400], AUC: 0.6416\n",
      "Epoch [5500], AUC: 0.6420\n",
      "Epoch [5600], AUC: 0.6417\n",
      "Epoch [5700], AUC: 0.6416\n",
      "Epoch [5800], AUC: 0.6417\n",
      "Epoch [5900], AUC: 0.6415\n",
      "Epoch [6000], AUC: 0.6420\n",
      "Epoch [6100], AUC: 0.6421\n",
      "Epoch [6200], AUC: 0.6417\n",
      "Epoch [6300], AUC: 0.6413\n",
      "Epoch [6400], AUC: 0.6411\n",
      "Epoch [6500], AUC: 0.6415\n",
      "Epoch [6600], AUC: 0.6413\n",
      "Epoch [6700], AUC: 0.6411\n",
      "Epoch [6800], AUC: 0.6413\n",
      "Epoch [6900], AUC: 0.6411\n",
      "Using GE  AUC 0.642\n",
      "8\n",
      "Epoch [100], AUC: 0.4778\n",
      "Epoch [200], AUC: 0.4853\n",
      "Epoch [300], AUC: 0.5064\n",
      "Epoch [400], AUC: 0.5016\n",
      "Epoch [500], AUC: 0.5015\n",
      "Epoch [600], AUC: 0.5071\n",
      "Epoch [700], AUC: 0.4909\n",
      "Epoch [800], AUC: 0.4877\n",
      "Epoch [900], AUC: 0.5160\n",
      "Epoch [1000], AUC: 0.5194\n",
      "Epoch [1100], AUC: 0.4781\n",
      "Epoch [1200], AUC: 0.5252\n",
      "Epoch [1300], AUC: 0.4719\n",
      "Epoch [1400], AUC: 0.5324\n",
      "Epoch [1500], AUC: 0.4657\n",
      "Epoch [1600], AUC: 0.5377\n",
      "Epoch [1700], AUC: 0.5406\n",
      "Epoch [1800], AUC: 0.5441\n",
      "Epoch [1900], AUC: 0.5474\n",
      "Epoch [2000], AUC: 0.5502\n",
      "Epoch [2100], AUC: 0.5529\n",
      "Epoch [2200], AUC: 0.4437\n",
      "Epoch [2300], AUC: 0.5586\n",
      "Epoch [2400], AUC: 0.5604\n",
      "Epoch [2500], AUC: 0.4378\n",
      "Epoch [2600], AUC: 0.5642\n",
      "Epoch [2700], AUC: 0.5665\n",
      "Epoch [2800], AUC: 0.5674\n",
      "Epoch [2900], AUC: 0.5689\n",
      "Epoch [3000], AUC: 0.5694\n",
      "Epoch [3100], AUC: 0.5724\n",
      "Epoch [3200], AUC: 0.5743\n",
      "Epoch [3300], AUC: 0.5749\n",
      "Epoch [3400], AUC: 0.5766\n",
      "Epoch [3500], AUC: 0.5785\n",
      "Epoch [3600], AUC: 0.5797\n",
      "Epoch [3700], AUC: 0.5809\n",
      "Epoch [3800], AUC: 0.5824\n",
      "Epoch [3900], AUC: 0.5832\n",
      "Epoch [4000], AUC: 0.5842\n",
      "Epoch [4100], AUC: 0.4151\n",
      "Epoch [4200], AUC: 0.5856\n",
      "Epoch [4300], AUC: 0.5870\n",
      "Epoch [4400], AUC: 0.5884\n",
      "Epoch [4500], AUC: 0.5895\n",
      "Epoch [4600], AUC: 0.5901\n",
      "Epoch [4700], AUC: 0.5912\n",
      "Epoch [4800], AUC: 0.5918\n",
      "Epoch [4900], AUC: 0.4406\n",
      "Epoch [5000], AUC: 0.5933\n",
      "Epoch [5100], AUC: 0.5946\n",
      "Epoch [5200], AUC: 0.4047\n",
      "Epoch [5300], AUC: 0.4477\n",
      "Epoch [5400], AUC: 0.5668\n",
      "Epoch [5500], AUC: 0.5973\n",
      "Epoch [5600], AUC: 0.4067\n",
      "Epoch [5700], AUC: 0.4023\n",
      "Epoch [5800], AUC: 0.4386\n",
      "Epoch [5900], AUC: 0.4266\n",
      "Epoch [6000], AUC: 0.4379\n",
      "Epoch [6100], AUC: 0.6036\n",
      "Epoch [6200], AUC: 0.3949\n",
      "Epoch [6300], AUC: 0.6060\n",
      "Epoch [6400], AUC: 0.3937\n",
      "Epoch [6500], AUC: 0.4160\n",
      "Epoch [6600], AUC: 0.3906\n",
      "Epoch [6700], AUC: 0.4500\n",
      "Epoch [6800], AUC: 0.3891\n",
      "Epoch [6900], AUC: 0.3888\n",
      "Epoch [7000], AUC: 0.5438\n",
      "Epoch [7100], AUC: 0.4178\n",
      "Epoch [7200], AUC: 0.5495\n",
      "Epoch [7300], AUC: 0.5871\n",
      "Epoch [7400], AUC: 0.3845\n",
      "Epoch [7500], AUC: 0.4234\n",
      "Epoch [7600], AUC: 0.4222\n",
      "Epoch [7700], AUC: 0.4266\n",
      "Epoch [7800], AUC: 0.5868\n",
      "Epoch [7900], AUC: 0.4113\n",
      "Epoch [8000], AUC: 0.5839\n",
      "Epoch [8100], AUC: 0.4186\n",
      "Epoch [8200], AUC: 0.3884\n",
      "Epoch [8300], AUC: 0.4028\n",
      "Epoch [8400], AUC: 0.3738\n",
      "Epoch [8500], AUC: 0.4057\n",
      "Epoch [8600], AUC: 0.5897\n",
      "Epoch [8700], AUC: 0.6208\n",
      "Epoch [8800], AUC: 0.3985\n",
      "Epoch [8900], AUC: 0.4071\n",
      "Epoch [9000], AUC: 0.4098\n",
      "Epoch [9100], AUC: 0.4006\n",
      "Epoch [9200], AUC: 0.4005\n",
      "Epoch [9300], AUC: 0.4014\n",
      "Epoch [9400], AUC: 0.4022\n",
      "Epoch [9500], AUC: 0.4121\n",
      "Epoch [9600], AUC: 0.3774\n",
      "Epoch [9700], AUC: 0.6398\n",
      "Epoch [9800], AUC: 0.5999\n",
      "Epoch [9900], AUC: 0.6405\n",
      "Epoch [10000], AUC: 0.6408\n",
      "Using GE  AUC 0.641\n",
      "9\n",
      "Best Hyperparameters ge2ad: {'learningRate': 0.0005, 'num_layer': 6, 'epoch': 1155}\n",
      "Best AUC: 0.6412411118293471\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "Best Hyperparameters ape: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best AUC: 0.6339510163039574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "yg_train = np.concatenate([ad_ge_ADNI,ad_ge_ANM1,ad_ge_ANM2,ad_ge_ROS])\n",
    "select_X_train=  np.concatenate([ADNI_value.T.values,ANM1_value.T.values,ANM2_value.T.values,ROS_value.T.values])\n",
    "SNP_train =np.concatenate([ADNI_SNP.values.reshape(-1,1),ANM1_SNP.values.reshape(-1,1),ANM2_SNP.values.reshape(-1,1), ROS_SNP.values.reshape(-1,1)])\n",
    "y_MMSE_train =np.concatenate([ADNI_MMSE.values.reshape(-1,1),ANM1_MMSE.values.reshape(-1,1),ANM2_MMSE.values.reshape(-1,1), ROS_MMSE.values.reshape(-1,1)])\n",
    "\n",
    "select_X_test= EW_value.T.values\n",
    "yg_test= ad_ge_EW\n",
    "SNP_test= EW_SNP.values.reshape(-1,1)\n",
    "y_MMSE_test=  EW_MMSE.values.astype('float').reshape(-1,1)\n",
    "\n",
    "y_MMSE_train= minmax_scale(y_MMSE_train)\n",
    "y_MMSE_test=minmax_scale(y_MMSE_test)\n",
    "\n",
    "SNP_train = np.concatenate([SNP_train.astype('int')//10,SNP_train.astype('int')%10],axis=1)\n",
    "SNP_test = np.concatenate([SNP_test.astype('int')//10,SNP_test.astype('int')%10],axis=1)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                        np.concatenate([select_X_train,y_MMSE_train ],axis=1), yg_train,\n",
    "                test_size = 0.20,stratify =yg_train, random_state = random_seed)\n",
    "\n",
    "SNP_train, SNP_val, y_train2, y_val2 = train_test_split(\n",
    "                        SNP_train, yg_train,\n",
    "                test_size = 0.20,stratify =yg_train, random_state = random_seed)\n",
    "\n",
    "y_MMSE_train = X_train[:,-1]\n",
    "y_MMSE_val = X_val[:,-1]\n",
    "\n",
    "X_train = X_train[:,:-1]\n",
    "X_val = X_val[:,:-1]\n",
    "\n",
    "print(y_val)\n",
    "print(y_val2)\n",
    "\n",
    "\n",
    "param_grid = {'learningRate' : [0.0005,0.00005,0.000005] , 'weightDecay': [0.0001], 'num_layer': [4,6,8],'batch': [0]} \n",
    "\n",
    "best_auc = 0  \n",
    "best_params = {} \n",
    "num = 0\n",
    "\n",
    "X_train = torch.tensor(X_train.astype('float'), dtype=torch.float32).cuda()\n",
    "X_val = torch.tensor(X_val.astype('float'), dtype=torch.float32).cuda()\n",
    "\n",
    "y_label_train2 = torch.tensor(y_train.astype('float'), dtype=torch.float32).cuda()\n",
    "y_MMSE_train = torch.tensor(y_MMSE_train.astype('float'), dtype=torch.float32).cuda()\n",
    "ds = TensorDataset(X_train,y_MMSE_train)\n",
    "results = []\n",
    "best_final_auc = 0\n",
    "\n",
    "for learningRate in param_grid['learningRate']:\n",
    "    for weightDecay in param_grid['weightDecay']:\n",
    "        for num_layer in param_grid['num_layer']:\n",
    "            for batch in param_grid['batch']:\n",
    "                num_epochs = 10000\n",
    "                max_eval_auc = 0 \n",
    "                best_auc = 0 \n",
    "                epoch_check = 0\n",
    "                patience_limit = 2000 \n",
    "                patience_check = 0 \n",
    "                if batch == 0 :\n",
    "                    loader  = DataLoader(ds, batch_size=1,shuffle=True)\n",
    "                else:\n",
    "                    loader  = DataLoader(ds, batch_size=X_train.shape[0],shuffle=True)\n",
    "                        \n",
    "                if num_layer == 4 :\n",
    "                    net = DNN4()\n",
    "                elif num_layer == 6:\n",
    "                    net = DNN6()\n",
    "                else:\n",
    "                    net = DNN8()\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                net = net.to(device)\n",
    "                \n",
    "                optimizer = optim.SGD(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "                loss_fn2 =nn.MSELoss()\n",
    "\n",
    "                for epoch in (range(num_epochs)):\n",
    "                    running_loss = 0.0\n",
    "                    for i, data in enumerate(loader, 0):\n",
    "                        xg,y1= data\n",
    "                        output1 = net.forward(xg)\n",
    "\n",
    "                        net.train()\n",
    "                        optimizer.zero_grad()\n",
    "                        if batch == 0 :\n",
    "                            loss =  loss_fn2(output1[0], y1)\n",
    "                        else:\n",
    "                            output1  = output1.squeeze()\n",
    "                            loss =  loss_fn2(output1, y1)\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                    net.eval() \n",
    "                    epoch = epoch+1\n",
    "                    eval_loss = 0\n",
    "                    test1 = net.forward(X_val.clone().detach())\n",
    "                    test1 = test1.cpu().detach().numpy()\n",
    "                    train1 = net.forward(X_train.clone().detach())\n",
    "                    train1 = train1.cpu().detach().numpy()\n",
    "                    model4 = SVC(gamma='auto', probability=True)\n",
    "                    model4 = model4.fit(train1 , y_label_train2.cpu().detach().numpy())\n",
    "                    eval_auc = roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])\n",
    "                    if (epoch+1) % 100 == 0 :\n",
    "                        print ('Epoch [{}], AUC: {:.4f}'.format(epoch+1 ,eval_auc))\n",
    "                    if best_auc >= eval_auc: \n",
    "                        patience_check += 1\n",
    "                        if patience_check > patience_limit: \n",
    "                            break\n",
    "                    else: \n",
    "                        best_auc = eval_auc\n",
    "                        patience_check = 0\n",
    "                    if max_eval_auc <= eval_auc:\n",
    "                        epoch_check = epoch\n",
    "                        max_eval_auc = eval_auc\n",
    "                        torch.save(net, '/checkpoint_best3.pt')\n",
    "                \n",
    "                net = torch.load('/checkpoint_best3.pt')\n",
    "                test1 = net.forward(X_val.clone().detach())\n",
    "                test1 = test1.cpu().detach().numpy()\n",
    "                train1 = net.forward(X_train.clone().detach())\n",
    "                train1 = train1.cpu().detach().numpy()\n",
    "                model4 = SVC(gamma='auto', probability=True)\n",
    "                model4 = model4.fit(train1 , y_label_train2.cpu().detach().numpy())\n",
    "                print (\"Using GE  AUC %.3f\" %(roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])))\n",
    "\n",
    "                auc = roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])\n",
    "                num=num+1\n",
    "                print(num)\n",
    "                results.append({'auc': auc,'learningRate': learningRate, 'num_layer': num_layer,  'epoch' : epoch_check})\n",
    "                if auc > best_final_auc:\n",
    "                    best_final_auc = auc\n",
    "                    best_params = {'learningRate': learningRate, 'num_layer': num_layer, 'epoch' : epoch_check }\n",
    "                    torch.save(net, '/checkpoint_best3_final.pt')\n",
    "\n",
    "print(\"Best Hyperparameters ge2ad:\", best_params)\n",
    "print(\"Best AUC:\", best_auc)\n",
    "\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "                \n",
    "\n",
    "best_auc = 0  \n",
    "best_params = {} \n",
    "num = 0\n",
    "for C in param_grid['C']:\n",
    "    for gamma in param_grid['gamma']:\n",
    "        for kernel in param_grid['kernel']:\n",
    "            svm_model = SVC(C=C, gamma=gamma, kernel=kernel, probability=True)\n",
    "            svm_model.fit(SNP_train, y_train)\n",
    "            y_pred_proba = svm_model.predict_proba(SNP_val)[:, 1]\n",
    "            auc = roc_auc_score(y_val, y_pred_proba)\n",
    "            num=num+1\n",
    "            print(num)\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_params = {'C': C, 'gamma': gamma, 'kernel': kernel}\n",
    "                \n",
    "\n",
    "print(\"Best Hyperparameters ape:\", best_params)\n",
    "print(\"Best AUC:\", best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GE  AUC 0.72751\n",
      "Using APOE4  AUC 0.69974\n",
      "Ensemble AUC 0.76190\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "net = torch.load('/checkpoint_best3_final.pt')\n",
    "test1 = net.forward(torch.tensor(select_X_test.astype('float'), dtype=torch.float32).cuda().clone().detach())\n",
    "test1 = test1.cpu().detach().numpy()\n",
    "\n",
    "train1 = net.forward(X_train.clone().detach())\n",
    "train1 = train1.cpu().detach().numpy()\n",
    "model4 = SVC(gamma='auto', probability=True)\n",
    "model4 = model4.fit(train1 , y_label_train2.cpu().detach().numpy())\n",
    "print (\"Using GE  AUC %.5f\" %(roc_auc_score(yg_test.reshape(-1),model4.predict_proba(test1)[:,1])))\n",
    "\n",
    "ml =   SVC( gamma=0.1, kernel='rbf',C = 1, probability=True)\n",
    "ml.fit(SNP_train, y_train)\n",
    "\n",
    "print(\"Using APOE4  AUC %.5f\" %(roc_auc_score(yg_test,list(ml.predict_proba(SNP_test)[:,1]))))\n",
    "print(\"Ensemble AUC %.5f\" %(roc_auc_score(yg_test,list(model4.predict_proba(test1)[:,1] + ml.predict_proba(SNP_test)[:,1]))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
