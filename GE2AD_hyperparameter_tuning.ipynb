{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "\n",
    "import pickle\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix,recall_score,precision_score,precision_recall_curve,f1_score,auc\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "import scipy.stats as ss\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN4, self).__init__()\n",
    "        self.dense1 =nn.Sequential(nn.Linear(14040 , int(14040 *1/32)))\n",
    "        self.dense2 =nn.Sequential(nn.Linear(int(14040 *1/32), int(14040 *1/512)))\n",
    "        self.dense4  =nn.Sequential(nn.Linear(int(14040 *1/512),int(14040 *1/8192)))\n",
    "        self.dense6 =nn.Sequential(nn.Linear(int(14040 *1/8192),1),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dense4(X)             \n",
    "        se_predict = self.dense6(X)\n",
    "\n",
    "        return se_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN6, self).__init__()\n",
    "        self.dense1 =nn.Sequential(nn.Linear(14040 , int(14040 *1/16)))\n",
    "        self.dense2 =nn.Sequential(nn.Linear(int(14040 *1/16), int(14040 *1/64)))\n",
    "        self.dense3 =nn.Sequential(nn.Linear(int(14040 *1/64),int(14040 *1/256)))\n",
    "        self.dense4  =nn.Sequential(nn.Linear(int(14040 *1/256),int(14040 *1/1024)))\n",
    "        self.dense5  =nn.Sequential(nn.Linear(int(14040 *1/1024),int(14040 *1/4096)))\n",
    "        self.dense6 =nn.Sequential(nn.Linear(int(14040 *1/4096),1),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dense3(X)\n",
    "        X = self.dense4(X)        \n",
    "        X = self.dense5(X)        \n",
    "        se_predict = self.dense6(X)\n",
    "\n",
    "        return se_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN8, self).__init__()\n",
    "        self.dense0 =nn.Sequential(nn.Linear(14040 , int(14040 *1/2)))\n",
    "        self.dense1 =nn.Sequential(nn.Linear(int(14040 *1/2) , int(14040 *1/8)))\n",
    "        self.dense12 =nn.Sequential(nn.Linear(int(14040 *1/8) , int(14040 *1/32)))\n",
    "        self.dense2 =nn.Sequential(nn.Linear(int(14040 *1/32), int(14040 *1/128)))\n",
    "        self.dense3 =nn.Sequential(nn.Linear(int(14040 *1/128),int(14040 *1/512)))\n",
    "        self.dense4  =nn.Sequential(nn.Linear(int(14040 *1/512),int(14040 *1/2048)))\n",
    "        self.dense5  =nn.Sequential(nn.Linear(int(14040 *1/2048),int(14040 *1/8192)))\n",
    "        self.dense6 =nn.Sequential(nn.Linear(int(14040 *1/8192),1),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.dense12(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.dense3(X)\n",
    "        X = self.dense4(X)        \n",
    "        X = self.dense5(X)        \n",
    "        se_predict = self.dense6(X)\n",
    "\n",
    "        return se_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/user_home/moons/anaconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ANM1 = pd.read_csv('/user_home/moons/EW/official/ANM1_inter_norm.csv',sep=',',index_col=0)\n",
    "ANM2 = pd.read_csv('/user_home/moons/EW/official/ANM2_inter_norm.csv',sep=',',index_col=0)\n",
    "ADNI = pd.read_csv('/user_home/moons/EW/official/ADNI_inter_norm.csv',sep=',',index_col=0)\n",
    "add1 = pd.read_csv('/user_home/moons/EW/official/EW_inter_norm.csv',sep=',',index_col=0)\n",
    "ROS = pd.read_csv('/user_home/moons/EW/official/ROSMAP_inter_norm.csv',sep=',',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1 = add1.loc[:,add1.loc['DX']!='MCI' ]\n",
    "annot = add1.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_EW = np.ravel(annot, order='F').astype('float32')\n",
    "EW_SNP = add1.loc['APOE4']\n",
    "EW_MMSE= add1.loc['MMSE']\n",
    "EW_value = add1\n",
    "EW_value.drop('APOE4',axis=0,inplace=True)\n",
    "EW_value.drop('DX',axis=0,inplace=True)\n",
    "EW_value.drop('MMSE',axis=0,inplace=True)\n",
    "\n",
    "\n",
    "ADNI = ADNI.loc[:,ADNI.loc['DX']!='MCI' ]\n",
    "ADNI = ADNI.dropna(axis=1)\n",
    "annot = ADNI.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_ADNI = np.ravel(annot, order='F').astype('float32')\n",
    "ADNI_SNP = ADNI.loc['APOE4']\n",
    "ADNI_MMSE= ADNI.loc['MMSE']        \n",
    "ADNI.drop('APOE4',axis=0,inplace=True)\n",
    "ADNI.drop('DX',axis=0,inplace=True)\n",
    "ADNI.drop('MMSE',axis=0,inplace=True)\n",
    "ADNI_value = ADNI\n",
    "\n",
    "ANM1 = ANM1.loc[:,ANM1.loc['DX']!='MCI' ]\n",
    "annot = ANM1.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_ANM1 = np.ravel(annot, order='F').astype('float32')\n",
    "ANM1_SNP = ANM1.loc['APOE4']\n",
    "ANM1_MMSE= ANM1.loc['MMSE']\n",
    "ANM1.drop('APOE4',axis=0,inplace=True)\n",
    "ANM1.drop('MMSE',axis=0,inplace=True)\n",
    "ANM1.drop('DX',axis=0,inplace=True)\n",
    "ANM1_value = ANM1\n",
    "\n",
    "ANM2 = ANM2.loc[:,ANM2.loc['DX']!='MCI' ]\n",
    "annot = ANM2.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "ad_ge_ANM2 = np.ravel(annot, order='F').astype('float32')\n",
    "ANM2.drop('DX',axis=0,inplace=True)\n",
    "ANM2.loc['AGE'] = pd.DataFrame(ANM2.loc['AGE']).fillna(pd.DataFrame(ANM2.loc['AGE']).astype('float').mean(axis=0)).T.loc['AGE']\n",
    "ANM2_SNP = ANM2.loc['APOE4']\n",
    "ANM2_MMSE= ANM2.loc['MMSE']\n",
    "ANM2.drop('APOE4',axis=0,inplace=True)\n",
    "ANM2.drop('MMSE',axis=0,inplace=True)\n",
    "ANM2_value = ANM2\n",
    "\n",
    "ROS = ROS.loc[:,ROS.loc['DX']!='MCI' ]\n",
    "annot = ROS.loc['DX'].replace('NL' ,0)\n",
    "annot = annot.replace('AD' ,1)\n",
    "\n",
    "ad_ge_ROS = np.ravel(annot, order='F').astype('float32')\n",
    "ROS_SNP = ROS.loc['APOE4']\n",
    "ROS_MMSE= ROS.loc['MMSE']\n",
    "ROS.drop('APOE4',axis=0,inplace=True)\n",
    "ROS.drop('MMSE',axis=0,inplace=True)\n",
    "ROS.drop('DX',axis=0,inplace=True)\n",
    "ROS_value = ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "yg_train = np.concatenate([ad_ge_ADNI,ad_ge_ANM1,ad_ge_ANM2,ad_ge_ROS])\n",
    "select_X_train=  np.concatenate([ADNI_value.T.values,ANM1_value.T.values,ANM2_value.T.values,ROS_value.T.values])\n",
    "SNP_train =np.concatenate([ADNI_SNP.values.reshape(-1,1),ANM1_SNP.values.reshape(-1,1),ANM2_SNP.values.reshape(-1,1), ROS_SNP.values.reshape(-1,1)])\n",
    "y_MMSE_train =np.concatenate([ADNI_MMSE.values.reshape(-1,1),ANM1_MMSE.values.reshape(-1,1),ANM2_MMSE.values.reshape(-1,1), ROS_MMSE.values.reshape(-1,1)])\n",
    "\n",
    "select_X_test= EW_value.T.values\n",
    "yg_test= ad_ge_EW\n",
    "SNP_test= EW_SNP.values.reshape(-1,1)\n",
    "y_MMSE_test=  EW_MMSE.values.astype('float').reshape(-1,1)\n",
    "\n",
    "y_MMSE_train= minmax_scale(y_MMSE_train)\n",
    "y_MMSE_test=minmax_scale(y_MMSE_test)\n",
    "\n",
    "SNP_train = np.concatenate([SNP_train.astype('int')//10,SNP_train.astype('int')%10],axis=1)\n",
    "SNP_test = np.concatenate([SNP_test.astype('int')//10,SNP_test.astype('int')%10],axis=1)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                        np.concatenate([select_X_train,y_MMSE_train ],axis=1), yg_train,\n",
    "                test_size = 0.20,stratify =yg_train, random_state = random_seed)\n",
    "\n",
    "SNP_train, SNP_val, y_train2, y_val2 = train_test_split(\n",
    "                        SNP_train, yg_train,\n",
    "                test_size = 0.20,stratify =yg_train, random_state = random_seed)\n",
    "\n",
    "y_MMSE_train = X_train[:,-1]\n",
    "y_MMSE_val = X_val[:,-1]\n",
    "\n",
    "X_train = X_train[:,:-1]\n",
    "X_val = X_val[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0.]\n",
      "Epoch [100], Loss: 0.4849\n",
      "  AUC 0.616\n",
      "Epoch [200], Loss: 0.4998\n",
      "  AUC 0.640\n",
      "Epoch [300], Loss: 0.5567\n",
      "  AUC 0.671\n",
      "Epoch [400], Loss: 0.4975\n",
      "  AUC 0.691\n",
      "Epoch [500], Loss: 0.5388\n",
      "  AUC 0.693\n",
      "Epoch [600], Loss: 0.4947\n",
      "  AUC 0.697\n",
      "Epoch [700], Loss: 0.4803\n",
      "  AUC 0.699\n",
      "Epoch [800], Loss: 0.5389\n",
      "  AUC 0.702\n",
      "Epoch [900], Loss: 0.5166\n",
      "  AUC 0.701\n",
      "Epoch [1000], Loss: 0.5055\n",
      "  AUC 0.702\n",
      "Epoch [1100], Loss: 0.4974\n",
      "  AUC 0.699\n",
      "Epoch [1200], Loss: 0.5176\n",
      "  AUC 0.701\n",
      "Epoch [1300], Loss: 0.5025\n",
      "  AUC 0.702\n",
      "Epoch [1400], Loss: 0.5259\n",
      "  AUC 0.701\n",
      "Epoch [1500], Loss: 0.5192\n",
      "  AUC 0.701\n",
      "Epoch [1600], Loss: 0.5255\n",
      "  AUC 0.700\n",
      "Epoch [1700], Loss: 0.5171\n",
      "  AUC 0.700\n",
      "Epoch [1800], Loss: 0.5264\n",
      "  AUC 0.703\n",
      "Epoch [1900], Loss: 0.5191\n",
      "  AUC 0.702\n",
      "Epoch [2000], Loss: 0.5270\n",
      "  AUC 0.701\n",
      "Epoch [2100], Loss: 0.5234\n",
      "  AUC 0.702\n",
      "Epoch [2200], Loss: 0.5187\n",
      "  AUC 0.703\n",
      "Epoch [2300], Loss: 0.4989\n",
      "  AUC 0.701\n",
      "Epoch [2400], Loss: 0.5223\n",
      "  AUC 0.703\n",
      "Epoch [2500], Loss: 0.5249\n",
      "  AUC 0.701\n",
      "Epoch [2600], Loss: 0.5253\n",
      "  AUC 0.701\n",
      "Epoch [2700], Loss: 0.5246\n",
      "  AUC 0.701\n",
      "Epoch [2800], Loss: 0.5039\n",
      "  AUC 0.703\n",
      "Epoch [2900], Loss: 0.5343\n",
      "  AUC 0.703\n",
      "Epoch [3000], Loss: 0.5241\n",
      "  AUC 0.702\n",
      "Epoch [3100], Loss: 0.5288\n",
      "  AUC 0.701\n",
      "Epoch [3200], Loss: 0.5253\n",
      "  AUC 0.702\n",
      "Epoch [3300], Loss: 0.5363\n",
      "  AUC 0.702\n",
      "Epoch [3400], Loss: 0.5292\n",
      "  AUC 0.702\n",
      "Epoch [3500], Loss: 0.5302\n",
      "  AUC 0.703\n",
      "Epoch [3600], Loss: 0.5394\n",
      "  AUC 0.704\n",
      "Epoch [3700], Loss: 0.5228\n",
      "  AUC 0.703\n",
      "Epoch [3800], Loss: 0.5158\n",
      "  AUC 0.702\n",
      "Epoch [3900], Loss: 0.5293\n",
      "  AUC 0.703\n",
      "Epoch [4000], Loss: 0.5268\n",
      "  AUC 0.702\n",
      "Epoch [4100], Loss: 0.5254\n",
      "  AUC 0.702\n",
      "Epoch [4200], Loss: 0.5243\n",
      "  AUC 0.703\n",
      "Epoch [4300], Loss: 0.5208\n",
      "  AUC 0.703\n",
      "Epoch [4400], Loss: 0.5261\n",
      "  AUC 0.702\n",
      "Epoch [4500], Loss: 0.5380\n",
      "  AUC 0.702\n",
      "Epoch [4600], Loss: 0.5205\n",
      "  AUC 0.703\n",
      "Epoch [4700], Loss: 0.5292\n",
      "  AUC 0.702\n",
      "Epoch [4800], Loss: 0.5262\n",
      "  AUC 0.702\n",
      "Epoch [4900], Loss: 0.5278\n",
      "  AUC 0.702\n",
      "Epoch [5000], Loss: 0.5271\n",
      "  AUC 0.702\n",
      "Epoch [5100], Loss: 0.5299\n",
      "  AUC 0.703\n",
      "Epoch [5200], Loss: 0.5294\n",
      "  AUC 0.703\n",
      "Epoch [5300], Loss: 0.5266\n",
      "  AUC 0.703\n",
      "Epoch [5400], Loss: 0.5246\n",
      "  AUC 0.699\n",
      "Epoch [5500], Loss: 0.5276\n",
      "  AUC 0.703\n",
      "Epoch [5600], Loss: 0.5280\n",
      "  AUC 0.703\n",
      "Epoch [5700], Loss: 0.5283\n",
      "  AUC 0.703\n",
      "Epoch [5800], Loss: 0.5261\n",
      "  AUC 0.703\n",
      "Epoch [5900], Loss: 0.5278\n",
      "  AUC 0.703\n",
      "Epoch [6000], Loss: 0.5411\n",
      "  AUC 0.703\n",
      "Epoch [6100], Loss: 0.5335\n",
      "  AUC 0.702\n",
      "Epoch [6200], Loss: 0.5264\n",
      "  AUC 0.704\n",
      "Epoch [6300], Loss: 0.5316\n",
      "  AUC 0.703\n",
      "Epoch [6400], Loss: 0.5287\n",
      "  AUC 0.702\n",
      "Epoch [6500], Loss: 0.5285\n",
      "  AUC 0.704\n",
      "Epoch [6600], Loss: 0.5231\n",
      "  AUC 0.703\n",
      "Epoch [6700], Loss: 0.5185\n",
      "  AUC 0.702\n",
      "Epoch [6800], Loss: 0.5264\n",
      "  AUC 0.703\n",
      "Epoch [6900], Loss: 0.5544\n",
      "  AUC 0.704\n",
      "Epoch [7000], Loss: 0.5314\n",
      "  AUC 0.704\n",
      "Epoch [7100], Loss: 0.5256\n",
      "  AUC 0.703\n",
      "Epoch [7200], Loss: 0.5331\n",
      "  AUC 0.703\n",
      "Epoch [7300], Loss: 0.5306\n",
      "  AUC 0.704\n",
      "Epoch [7400], Loss: 0.5283\n",
      "  AUC 0.702\n",
      "Epoch [7500], Loss: 0.5310\n",
      "  AUC 0.702\n",
      "Epoch [7600], Loss: 0.5292\n",
      "  AUC 0.702\n",
      "Epoch [7700], Loss: 0.5284\n",
      "  AUC 0.703\n",
      "Epoch [7800], Loss: 0.5289\n",
      "  AUC 0.703\n",
      "Epoch [7900], Loss: 0.5277\n",
      "  AUC 0.706\n",
      "Epoch [8000], Loss: 0.5259\n",
      "  AUC 0.702\n",
      "Epoch [8100], Loss: 0.5314\n",
      "  AUC 0.704\n",
      "Epoch [8200], Loss: 0.4826\n",
      "  AUC 0.702\n",
      "Epoch [8300], Loss: 0.5305\n",
      "  AUC 0.702\n",
      "Epoch [8400], Loss: 0.5304\n",
      "  AUC 0.704\n",
      "Epoch [8500], Loss: 0.5330\n",
      "  AUC 0.704\n",
      "Epoch [8600], Loss: 0.5356\n",
      "  AUC 0.704\n",
      "Epoch [8700], Loss: 0.5383\n",
      "  AUC 0.703\n",
      "Epoch [8800], Loss: 0.5302\n",
      "  AUC 0.703\n",
      "Epoch [8900], Loss: 0.5264\n",
      "  AUC 0.703\n",
      "Epoch [9000], Loss: 0.6077\n",
      "  AUC 0.704\n",
      "Epoch [9100], Loss: 0.5244\n",
      "  AUC 0.703\n",
      "Epoch [9200], Loss: 0.5345\n",
      "  AUC 0.702\n",
      "Epoch [9300], Loss: 0.5332\n",
      "  AUC 0.702\n",
      "Epoch [9400], Loss: 0.5312\n",
      "  AUC 0.701\n",
      "Epoch [9500], Loss: 0.5294\n",
      "  AUC 0.705\n",
      "Epoch [9600], Loss: 0.5264\n",
      "  AUC 0.702\n",
      "Epoch [9700], Loss: 0.5271\n",
      "  AUC 0.703\n",
      "Epoch [9800], Loss: 0.5252\n",
      "  AUC 0.702\n",
      "Epoch [9900], Loss: 0.5261\n",
      "  AUC 0.701\n",
      "Epoch [10000], Loss: 0.5367\n",
      "  AUC 0.702\n",
      "Using GE  AUC 0.706\n",
      "1\n",
      "Epoch [100], Loss: 0.4442\n",
      "  AUC 0.616\n",
      "Epoch [200], Loss: 0.5200\n",
      "  AUC 0.633\n",
      "Epoch [300], Loss: 0.5114\n",
      "  AUC 0.669\n",
      "Epoch [400], Loss: 0.5176\n",
      "  AUC 0.688\n",
      "Epoch [500], Loss: 0.4959\n",
      "  AUC 0.696\n",
      "Epoch [600], Loss: 0.5025\n",
      "  AUC 0.700\n",
      "Epoch [700], Loss: 0.5007\n",
      "  AUC 0.702\n",
      "Epoch [800], Loss: 0.5312\n",
      "  AUC 0.699\n",
      "Epoch [900], Loss: 0.5245\n",
      "  AUC 0.702\n",
      "Epoch [1000], Loss: 0.5542\n",
      "  AUC 0.703\n",
      "Epoch [1100], Loss: 0.4856\n",
      "  AUC 0.701\n",
      "Epoch [1200], Loss: 0.5388\n",
      "  AUC 0.703\n",
      "Epoch [1300], Loss: 0.5233\n",
      "  AUC 0.705\n",
      "Epoch [1400], Loss: 0.5365\n",
      "  AUC 0.703\n",
      "Epoch [1500], Loss: 0.5307\n",
      "  AUC 0.704\n",
      "Epoch [1600], Loss: 0.5251\n",
      "  AUC 0.702\n",
      "Epoch [1700], Loss: 0.5176\n",
      "  AUC 0.704\n",
      "Epoch [1800], Loss: 0.5312\n",
      "  AUC 0.706\n",
      "Epoch [1900], Loss: 0.5121\n",
      "  AUC 0.703\n",
      "Epoch [2000], Loss: 0.5635\n",
      "  AUC 0.705\n",
      "Epoch [2100], Loss: 0.5309\n",
      "  AUC 0.702\n",
      "Epoch [2200], Loss: 0.5233\n",
      "  AUC 0.705\n",
      "Epoch [2300], Loss: 0.5141\n",
      "  AUC 0.704\n",
      "Epoch [2400], Loss: 0.5266\n",
      "  AUC 0.703\n",
      "Epoch [2500], Loss: 0.5263\n",
      "  AUC 0.704\n",
      "Epoch [2600], Loss: 0.5224\n",
      "  AUC 0.704\n",
      "Epoch [2700], Loss: 0.5234\n",
      "  AUC 0.705\n",
      "Epoch [2800], Loss: 0.5218\n",
      "  AUC 0.705\n",
      "Epoch [2900], Loss: 0.5213\n",
      "  AUC 0.704\n",
      "Epoch [3000], Loss: 0.5269\n",
      "  AUC 0.705\n",
      "Epoch [3100], Loss: 0.5218\n",
      "  AUC 0.704\n",
      "Epoch [3200], Loss: 0.5304\n",
      "  AUC 0.705\n",
      "Epoch [3300], Loss: 0.5370\n",
      "  AUC 0.706\n",
      "Epoch [3400], Loss: 0.5277\n",
      "  AUC 0.704\n",
      "Epoch [3500], Loss: 0.5256\n",
      "  AUC 0.704\n",
      "Epoch [3600], Loss: 0.5244\n",
      "  AUC 0.704\n",
      "Epoch [3700], Loss: 0.5262\n",
      "  AUC 0.705\n",
      "Epoch [3800], Loss: 0.5275\n",
      "  AUC 0.703\n",
      "Epoch [3900], Loss: 0.5270\n",
      "  AUC 0.704\n",
      "Epoch [4000], Loss: 0.5247\n",
      "  AUC 0.705\n",
      "Epoch [4100], Loss: 0.5234\n",
      "  AUC 0.703\n",
      "Epoch [4200], Loss: 0.5228\n",
      "  AUC 0.704\n",
      "Epoch [4300], Loss: 0.5216\n",
      "  AUC 0.704\n",
      "Epoch [4400], Loss: 0.5421\n",
      "  AUC 0.705\n",
      "Epoch [4500], Loss: 0.5478\n",
      "  AUC 0.704\n",
      "Epoch [4600], Loss: 0.5197\n",
      "  AUC 0.703\n",
      "Epoch [4700], Loss: 0.5257\n",
      "  AUC 0.704\n",
      "Epoch [4800], Loss: 0.5295\n",
      "  AUC 0.704\n",
      "Epoch [4900], Loss: 0.5281\n",
      "  AUC 0.702\n",
      "Epoch [5000], Loss: 0.5242\n",
      "  AUC 0.703\n",
      "Epoch [5100], Loss: 0.5280\n",
      "  AUC 0.705\n",
      "Epoch [5200], Loss: 0.5319\n",
      "  AUC 0.704\n",
      "Epoch [5300], Loss: 0.5184\n",
      "  AUC 0.702\n",
      "Epoch [5400], Loss: 0.5315\n",
      "  AUC 0.702\n",
      "Epoch [5500], Loss: 0.5279\n",
      "  AUC 0.702\n",
      "Epoch [5600], Loss: 0.5301\n",
      "  AUC 0.703\n",
      "Epoch [5700], Loss: 0.5621\n",
      "  AUC 0.702\n",
      "Epoch [5800], Loss: 0.5279\n",
      "  AUC 0.703\n",
      "Epoch [5900], Loss: 0.5300\n",
      "  AUC 0.703\n",
      "Epoch [6000], Loss: 0.5287\n",
      "  AUC 0.703\n",
      "Epoch [6100], Loss: 0.4985\n",
      "  AUC 0.702\n",
      "Epoch [6200], Loss: 0.5239\n",
      "  AUC 0.703\n",
      "Epoch [6300], Loss: 0.5261\n",
      "  AUC 0.703\n",
      "Epoch [6400], Loss: 0.5298\n",
      "  AUC 0.699\n",
      "Epoch [6500], Loss: 0.5235\n",
      "  AUC 0.703\n",
      "Epoch [6600], Loss: 0.5289\n",
      "  AUC 0.703\n",
      "Epoch [6700], Loss: 0.5392\n",
      "  AUC 0.702\n",
      "Epoch [6800], Loss: 0.5332\n",
      "  AUC 0.704\n",
      "Epoch [6900], Loss: 0.5309\n",
      "  AUC 0.702\n",
      "Epoch [7000], Loss: 0.5282\n",
      "  AUC 0.702\n",
      "Epoch [7100], Loss: 0.5284\n",
      "  AUC 0.702\n",
      "Epoch [7200], Loss: 0.5282\n",
      "  AUC 0.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7300], Loss: 0.5281\n",
      "  AUC 0.702\n",
      "Epoch [7400], Loss: 0.5255\n",
      "  AUC 0.703\n",
      "Epoch [7500], Loss: 0.5267\n",
      "  AUC 0.703\n",
      "Epoch [7600], Loss: 0.5253\n",
      "  AUC 0.703\n",
      "Epoch [7700], Loss: 0.5339\n",
      "  AUC 0.701\n",
      "Epoch [7800], Loss: 0.5334\n",
      "  AUC 0.701\n",
      "Epoch [7900], Loss: 0.5288\n",
      "  AUC 0.701\n",
      "Epoch [8000], Loss: 0.5308\n",
      "  AUC 0.701\n",
      "Epoch [8100], Loss: 0.5205\n",
      "  AUC 0.703\n",
      "Epoch [8200], Loss: 0.5294\n",
      "  AUC 0.703\n",
      "Epoch [8300], Loss: 0.5290\n",
      "  AUC 0.702\n",
      "Epoch [8400], Loss: 0.5985\n",
      "  AUC 0.700\n",
      "Epoch [8500], Loss: 0.5321\n",
      "  AUC 0.702\n",
      "Epoch [8600], Loss: 0.5187\n",
      "  AUC 0.701\n",
      "Epoch [8700], Loss: 0.5344\n",
      "  AUC 0.701\n",
      "Epoch [8800], Loss: 0.5316\n",
      "  AUC 0.702\n",
      "Epoch [8900], Loss: 0.5258\n",
      "  AUC 0.701\n",
      "Epoch [9000], Loss: 0.5353\n",
      "  AUC 0.701\n",
      "Epoch [9100], Loss: 0.5243\n",
      "  AUC 0.702\n",
      "Epoch [9200], Loss: 0.5307\n",
      "  AUC 0.702\n",
      "Epoch [9300], Loss: 0.5315\n",
      "  AUC 0.702\n",
      "Epoch [9400], Loss: 0.5309\n",
      "  AUC 0.702\n",
      "Epoch [9500], Loss: 0.6120\n",
      "  AUC 0.701\n",
      "Epoch [9600], Loss: 0.5295\n",
      "  AUC 0.702\n",
      "Epoch [9700], Loss: 0.5238\n",
      "  AUC 0.701\n",
      "Epoch [9800], Loss: 0.5360\n",
      "  AUC 0.701\n",
      "Epoch [9900], Loss: 0.5268\n",
      "  AUC 0.702\n",
      "Epoch [10000], Loss: 0.5351\n",
      "  AUC 0.701\n",
      "Using GE  AUC 0.707\n",
      "2\n",
      "Epoch [100], Loss: 0.4387\n",
      "  AUC 0.626\n",
      "Epoch [200], Loss: 0.4031\n",
      "  AUC 0.615\n",
      "Epoch [300], Loss: 0.5428\n",
      "  AUC 0.649\n",
      "Epoch [400], Loss: 0.4920\n",
      "  AUC 0.680\n",
      "Epoch [500], Loss: 0.5222\n",
      "  AUC 0.694\n",
      "Epoch [600], Loss: 0.5395\n",
      "  AUC 0.699\n",
      "Epoch [700], Loss: 0.5182\n",
      "  AUC 0.697\n",
      "Epoch [800], Loss: 0.5321\n",
      "  AUC 0.700\n",
      "Epoch [900], Loss: 0.5036\n",
      "  AUC 0.701\n",
      "Epoch [1000], Loss: 0.5091\n",
      "  AUC 0.698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "yg_train = np.concatenate([ad_ge_ADNI,ad_ge_ANM1,ad_ge_ANM2,ad_ge_ROS])\n",
    "select_X_train=  np.concatenate([ADNI_value.T.values,ANM1_value.T.values,ANM2_value.T.values,ROS_value.T.values])\n",
    "SNP_train =np.concatenate([ADNI_SNP.values.reshape(-1,1),ANM1_SNP.values.reshape(-1,1),ANM2_SNP.values.reshape(-1,1), ROS_SNP.values.reshape(-1,1)])\n",
    "y_MMSE_train =np.concatenate([ADNI_MMSE.values.reshape(-1,1),ANM1_MMSE.values.reshape(-1,1),ANM2_MMSE.values.reshape(-1,1), ROS_MMSE.values.reshape(-1,1)])\n",
    "\n",
    "select_X_test= EW_value.T.values\n",
    "yg_test= ad_ge_EW\n",
    "SNP_test= EW_SNP.values.reshape(-1,1)\n",
    "y_MMSE_test=  EW_MMSE.values.astype('float').reshape(-1,1)\n",
    "\n",
    "y_MMSE_train= minmax_scale(y_MMSE_train)\n",
    "y_MMSE_test=minmax_scale(y_MMSE_test)\n",
    "\n",
    "SNP_train = np.concatenate([SNP_train.astype('int')//10,SNP_train.astype('int')%10],axis=1)\n",
    "SNP_test = np.concatenate([SNP_test.astype('int')//10,SNP_test.astype('int')%10],axis=1)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                        np.concatenate([select_X_train,y_MMSE_train ],axis=1), yg_train,\n",
    "                test_size = 0.20,stratify =yg_train, random_state = random_seed)\n",
    "\n",
    "SNP_train, SNP_val, y_train2, y_val2 = train_test_split(\n",
    "                        SNP_train, yg_train,\n",
    "                test_size = 0.20,stratify =yg_train, random_state = random_seed)\n",
    "\n",
    "y_MMSE_train = X_train[:,-1]\n",
    "y_MMSE_val = X_val[:,-1]\n",
    "\n",
    "X_train = X_train[:,:-1]\n",
    "X_val = X_val[:,:-1]\n",
    "\n",
    "print(y_val)\n",
    "print(y_val2)\n",
    "\n",
    "\n",
    "param_grid = {'learningRate' : [0.0005,0.00005,0.000005] , 'weightDecay': [0.0001], 'num_layer': [4,6,8],'batch': [0]} \n",
    "\n",
    "best_auc = 0  \n",
    "best_params = {}\n",
    "num = 0\n",
    "\n",
    "X_train = torch.tensor(X_train.astype('float'), dtype=torch.float32).cuda()\n",
    "X_val = torch.tensor(X_val.astype('float'), dtype=torch.float32).cuda()\n",
    "\n",
    "y_label_train2 = torch.tensor(y_train.astype('float'), dtype=torch.float32).cuda()\n",
    "y_MMSE_train = torch.tensor(y_MMSE_train.astype('float'), dtype=torch.float32).cuda()\n",
    "ds = TensorDataset(X_train,y_MMSE_train)\n",
    "results = []\n",
    "\n",
    "for learningRate in param_grid['learningRate']:\n",
    "    for weightDecay in param_grid['weightDecay']:\n",
    "        for num_layer in param_grid['num_layer']:\n",
    "            for batch in param_grid['batch']:\n",
    "                num_epochs = 10000\n",
    "                min_eval_auc = 0 \n",
    "                best_auc = 0 \n",
    "                patience_limit = 10000 \n",
    "                patience_check = 0 \n",
    "\n",
    "                if batch == 0 :\n",
    "                    loader  = DataLoader(ds, batch_size=1,shuffle=True)\n",
    "                else:\n",
    "                    loader  = DataLoader(ds, batch_size=X_train.shape[0],shuffle=True)\n",
    "                        \n",
    "                if num_layer == 4 :\n",
    "                    net = DNN4()\n",
    "                elif num_layer == 6:\n",
    "                    net = DNN6()\n",
    "                else:\n",
    "                    net = DNN8()\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                net = net.to(device)\n",
    "                \n",
    "                optimizer = optim.SGD(net.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "                loss_fn2 =nn.MSELoss()\n",
    "\n",
    "                for epoch in (range(num_epochs)):\n",
    "                    running_loss = 0.0\n",
    "                    for i, data in enumerate(loader, 0):\n",
    "                        xg,y1= data\n",
    "                        output1 = net.forward(xg)\n",
    "\n",
    "                        net.train()\n",
    "                        optimizer.zero_grad()\n",
    "                        if batch == 0 :\n",
    "                            loss =  loss_fn2(output1[0], y1)\n",
    "                        else:\n",
    "                            output1  = output1.squeeze()\n",
    "                            loss =  loss_fn2(output1, y1)\n",
    "                        loss.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                    net.eval() \n",
    "                    epoch = epoch+1\n",
    "                    eval_loss = 0\n",
    "                    if (epoch+1) % 100 == 0 :\n",
    "                        test1 = net.forward(X_val.clone().detach())\n",
    "                        print ('Epoch [{}], Loss: {:.4f}'.format(epoch+1 ,loss_fn2(test1.squeeze(), torch.tensor(y_val.astype('float'), dtype=torch.float32).cuda())))\n",
    "                        eval_loss = loss_fn2(test1.squeeze(), torch.tensor(y_val.astype('float'), dtype=torch.float32).cuda())\n",
    "                        test1 = test1.cpu().detach().numpy()\n",
    "                        train1 = net.forward(X_train.clone().detach())\n",
    "                        train1 = train1.cpu().detach().numpy()\n",
    "                        model4 = SVC(gamma='auto', probability=True)\n",
    "                        model4 = model4.fit(train1 , y_label_train2.cpu().detach().numpy())\n",
    "        \n",
    "                        print (\"  AUC %.3f\" %(roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])))\n",
    "                    test1 = net.forward(X_val.clone().detach())\n",
    "                    test1 = test1.cpu().detach().numpy()\n",
    "                    train1 = net.forward(X_train.clone().detach())\n",
    "                    train1 = train1.cpu().detach().numpy()\n",
    "                    model4 = SVC(gamma='auto', probability=True)\n",
    "                    model4 = model4.fit(train1 , y_label_train2.cpu().detach().numpy())\n",
    "                    eval_auc = roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])\n",
    "                    if best_auc >= eval_auc: \n",
    "                        patience_check += 1\n",
    "\n",
    "                        if patience_check > patience_limit: \n",
    "                            break\n",
    "\n",
    "                    else: # loss가 개선된 경우\n",
    "                        best_auc = eval_auc\n",
    "                        patience_check = 0\n",
    "\n",
    "                    if min_eval_auc < best_auc:\n",
    "\n",
    "                        min_eval_auc = eval_auc\n",
    "                        torch.save(net, '/official/checkpoint_best.pt')\n",
    "                \n",
    "                net = torch.load('/official/checkpoint_best.pt')\n",
    "                test1 = net.forward(X_val.clone().detach())\n",
    "                test1 = test1.cpu().detach().numpy()\n",
    "                train1 = net.forward(X_train.clone().detach())\n",
    "                train1 = train1.cpu().detach().numpy()\n",
    "                model4 = SVC(gamma='auto', probability=True)\n",
    "                model4 = model4.fit(train1 , y_label_train2.cpu().detach().numpy())\n",
    "                print (\"Using GE  AUC %.3f\" %(roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])))\n",
    "\n",
    "                auc = roc_auc_score(y_val.reshape(-1),model4.predict_proba(test1)[:,1])\n",
    "                num=num+1\n",
    "                print(num)\n",
    "                results.append({'auc': auc,'learningRate': learningRate, 'weightDecay': weightDecay, 'num_layer': num_layer, 'batch': batch})\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    best_params = {'learningRate': learningRate, 'weightDecay': weightDecay, 'num_layer': num_layer, 'batch': batch}\n",
    "                \n",
    "                \n",
    "\n",
    "print(\"Best Hyperparameters ml:\", best_params)\n",
    "print(\"Best AUC:\", best_auc)\n",
    "\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "                \n",
    "\n",
    "best_auc = 0  \n",
    "best_params = {}  \n",
    "num = 0\n",
    "for C in param_grid['C']:\n",
    "    for gamma in param_grid['gamma']:\n",
    "        for kernel in param_grid['kernel']:\n",
    "\n",
    "            svm_model = SVC(C=C, gamma=gamma, kernel=kernel, probability=True)\n",
    "            svm_model.fit(SNP_train, y_train)\n",
    "            y_pred_proba = svm_model.predict_proba(SNP_val)[:, 1]\n",
    "            auc = roc_auc_score(y_val, y_pred_proba)\n",
    "            num=num+1\n",
    "            print(num)\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_params = {'C': C, 'gamma': gamma, 'kernel': kernel}\n",
    "                \n",
    "\n",
    "print(\"Best Hyperparameters ape:\", best_params)\n",
    "print(\"Best AUC:\", best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_model = SVC( gamma=0.0001, kernel='rbf',C =  0.1, probability=True)\n",
    "selection_model.fit(X_train, y_train)\n",
    "ml =   SVC( gamma=0.1, kernel='rbf',C =  0.1, probability=True)\n",
    "ml.fit(SNP_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Using SVM  AUC %.5f\" %(roc_auc_score(yg_test,selection_model.predict_proba(select_X_test)[:,1].astype(float))))\n",
    "print(\"Using APOE4  AUC %.5f\" %(roc_auc_score(yg_test,list(ml.predict_proba(SNP_test)[:,1]))))\n",
    "print(\"Ensemble AUC %.5f\" %(roc_auc_score(yg_test,list(ss.zscore(selection_model.predict_proba(select_X_test)[:,1].astype(float)) + ss.zscore(ml.predict_proba(SNP_test)[:,1])))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
